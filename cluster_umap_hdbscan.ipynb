{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f1e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3y - %60+ coverage: 70/70 (weeks=157)\n",
      "2y - %60+ coverage: 70/70 (weeks=105)\n",
      "6m - %60+ coverage: 70/70 (weeks=27)\n",
      "Shapes:\n",
      "df_3y: (70, 108) | df_2y: (70, 108) | df_6m: (70, 108)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.read_csv(\"data/processed/labeled_time_series.csv\")\n",
    "df_cleaned[\"date\"] = pd.to_datetime(df_cleaned[\"date\"])\n",
    "df_cleaned.head()\n",
    "\n",
    "# 0) date format\n",
    "df_cleaned[\"date\"] = pd.to_datetime(df_cleaned[\"date\"])\n",
    "\n",
    "# 1) feature cols\n",
    "META = {\"Country\", \"date\"}\n",
    "feature_cols = [c for c in df_cleaned.columns if c not in META]\n",
    "\n",
    "# 2) rolling cutoffs\n",
    "max_date = df_cleaned[\"date\"].max()\n",
    "cutoff_3y = max_date - pd.DateOffset(years=3)\n",
    "cutoff_2y = max_date - pd.DateOffset(years=2)\n",
    "cutoff_6m = max_date - pd.DateOffset(months=6)\n",
    "\n",
    "df_3y_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_3y].copy()\n",
    "df_2y_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_2y].copy()\n",
    "df_6m_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_6m].copy()\n",
    "\n",
    "# 3) coverage check (%60+ week coverage)\n",
    "def check_coverage(df_raw, name, threshold=0.6):\n",
    "    total_weeks = df_raw[\"date\"].nunique()\n",
    "    cov = df_raw.groupby(\"Country\")[\"date\"].nunique() / total_weeks\n",
    "    valid = cov[cov >= threshold].index\n",
    "    print(f\"{name} - %{int(threshold*100)}+ coverage: {len(valid)}/{df_raw['Country'].nunique()} (weeks={total_weeks})\")\n",
    "    return valid\n",
    "\n",
    "valid_3y = check_coverage(df_3y_raw, \"3y\")\n",
    "valid_2y = check_coverage(df_2y_raw, \"2y\")\n",
    "valid_6m = check_coverage(df_6m_raw, \"6m\")\n",
    "\n",
    "# 4) aggregate to 1 row per country (median recommended for trends)\n",
    "AGG = \"median\"   # \"mean\" is also ok\n",
    "\n",
    "def agg_country(df_raw, valid, agg=\"median\"):\n",
    "    df_raw = df_raw[df_raw[\"Country\"].isin(valid)]\n",
    "    if agg == \"median\":\n",
    "        return df_raw.groupby(\"Country\")[feature_cols].median().reset_index()\n",
    "    elif agg == \"mean\":\n",
    "        return df_raw.groupby(\"Country\")[feature_cols].mean().reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"agg must be 'median' or 'mean'\")\n",
    "\n",
    "df_3y = agg_country(df_3y_raw, valid_3y, agg=AGG)\n",
    "df_2y = agg_country(df_2y_raw, valid_2y, agg=AGG)\n",
    "df_6m = agg_country(df_6m_raw, valid_6m, agg=AGG)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"df_3y:\", df_3y.shape, \"| df_2y:\", df_2y.shape, \"| df_6m:\", df_6m.shape)\n",
    "\n",
    "datasets = {\"3y\": df_3y, \"2y\": df_2y, \"6m\": df_6m}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ede91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3Y WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/.conda/envs/trendyol_datathon/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 2 clusters, noise=0\n",
      "Optimal KMeans: k=2, silhouette=0.617\n",
      "0    62\n",
      "1     8\n",
      "Name: count, dtype: int64\n",
      "✓ Rapor dosyaları oluşturuldu (3y):\n",
      "   - cluster_profiles_zscores_3y.csv\n",
      "   - cluster_summary_top_features_3y.csv\n",
      "   - countries_per_cluster_3y.json\n",
      "\n",
      "=== 2Y WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/.conda/envs/trendyol_datathon/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 2 clusters, noise=0\n",
      "Optimal KMeans: k=2, silhouette=0.536\n",
      "0    61\n",
      "1     9\n",
      "Name: count, dtype: int64\n",
      "✓ Rapor dosyaları oluşturuldu (2y):\n",
      "   - cluster_profiles_zscores_2y.csv\n",
      "   - cluster_summary_top_features_2y.csv\n",
      "   - countries_per_cluster_2y.json\n",
      "\n",
      "=== 6M WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hosafxd/.conda/envs/trendyol_datathon/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 3 clusters, noise=0\n",
      "Optimal KMeans: k=4, silhouette=0.435\n",
      "0    11\n",
      "1    21\n",
      "2    24\n",
      "3    14\n",
      "Name: count, dtype: int64\n",
      "✓ Rapor dosyaları oluşturuldu (6m):\n",
      "   - cluster_profiles_zscores_6m.csv\n",
      "   - cluster_summary_top_features_6m.csv\n",
      "   - countries_per_cluster_6m.json\n",
      "\n",
      "Tüm işlemler tamamlandı! Çıktılar: data/processed/clustering_umap_hdbscan\n"
     ]
    }
   ],
   "source": [
    "# CELL2 - CLUSTERING + TAM RAPORLAMA (TOP FEATURES, COUNTRIES PER CLUSTER, Z-SCORES)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap\n",
    "import hdbscan\n",
    "import json\n",
    "\n",
    "OUT_DIR = \"data/processed/clustering_umap_hdbscan\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "final_results_hdb = {}\n",
    "final_results_km = {}\n",
    "reports = {}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n=== {name.upper()} WINDOW ===\")\n",
    "    \n",
    "    # Features\n",
    "    assert \"Country\" in data.columns\n",
    "    feat_cols = [c for c in data.columns if c != \"Country\"]\n",
    "    \n",
    "    # Numeric + missing handling\n",
    "    X_raw = data[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = X_raw.fillna(X_raw.median(numeric_only=True)).values\n",
    "    \n",
    "    # Scaling\n",
    "    X_scaled = RobustScaler().fit_transform(X)\n",
    "    \n",
    "    # UMAP\n",
    "    n_samples, n_features = X_scaled.shape\n",
    "    n_neighbors = min(15, max(2, n_samples - 1))\n",
    "    n_components = min(10, n_features)\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        min_dist=0.1,\n",
    "        random_state=42,\n",
    "        metric=\"euclidean\"\n",
    "    )\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    print(f\"UMAP: {X_umap.shape} (neighbors={n_neighbors}, components={n_components})\")\n",
    "    \n",
    "    # HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=5, min_samples=3, cluster_selection_method=\"eom\")\n",
    "    labels_hdb = clusterer.fit_predict(X_umap)\n",
    "    n_clusters_hdb = len(set(labels_hdb)) - (1 if -1 in labels_hdb else 0)\n",
    "    n_noise = int(np.sum(labels_hdb == -1))\n",
    "    print(f\"HDBSCAN: {n_clusters_hdb} clusters, noise={n_noise}\")\n",
    "    \n",
    "    out_hdb = data[[\"Country\"]].copy()\n",
    "    out_hdb[\"cluster_hdbscan\"] = labels_hdb\n",
    "    final_results_hdb[name] = out_hdb\n",
    "    \n",
    "    # Optimal K-Means on UMAP\n",
    "    k_min = 2\n",
    "    k_max = min(15, n_samples - 1)\n",
    "    km_best_k = None\n",
    "    km_best_sil = None\n",
    "    labels_km_best = None\n",
    "    \n",
    "    if k_max >= k_min:\n",
    "        K = list(range(k_min, k_max + 1))\n",
    "        sils = []\n",
    "        for k in K:\n",
    "            km = KMeans(n_clusters=k, random_state=42, n_init=25)\n",
    "            labels = km.fit_predict(X_umap)\n",
    "            sils.append(float(silhouette_score(X_umap, labels)))\n",
    "        \n",
    "        best_idx = int(np.argmax(sils))\n",
    "        km_best_k = K[best_idx]\n",
    "        km_best_sil = sils[best_idx]\n",
    "        \n",
    "        km_best = KMeans(n_clusters=km_best_k, random_state=42, n_init=50)\n",
    "        labels_km_best = km_best.fit_predict(X_umap)\n",
    "        \n",
    "        print(f\"Optimal KMeans: k={km_best_k}, silhouette={km_best_sil:.3f}\")\n",
    "        print(pd.Series(labels_km_best).value_counts().sort_index())\n",
    "    else:\n",
    "        print(\"KMeans skipped: not enough countries.\")\n",
    "        labels_km_best = np.zeros(n_samples)\n",
    "        km_best_k = 1\n",
    "        km_best_sil = 0.0\n",
    "    \n",
    "    out_km = data[[\"Country\"]].copy()\n",
    "    out_km[\"cluster_kmeans_umap\"] = labels_km_best\n",
    "    final_results_km[name] = out_km\n",
    "    \n",
    "    # ==================== YENİ: TAM RAPORLAMA ====================\n",
    "    \n",
    "    # 1) cluster_profiles_zscores.csv (z-score ortalamaları)\n",
    "    X_scaled_df = pd.DataFrame(X_scaled, columns=feat_cols)\n",
    "    X_scaled_df[\"cluster\"] = labels_km_best\n",
    "    cluster_zscores = X_scaled_df.groupby(\"cluster\")[feat_cols].mean()\n",
    "    cluster_zscores.to_csv(os.path.join(OUT_DIR, f\"cluster_profiles_zscores_{name}.csv\"))\n",
    "    \n",
    "    # 2) cluster_summary_top_features.csv (top 8 pos/neg + ülke sayısı)\n",
    "    TOP_N = 8\n",
    "    summary_rows = []\n",
    "    for cl in cluster_zscores.index:\n",
    "        s = cluster_zscores.loc[cl].sort_values(ascending=False)\n",
    "        top_pos = s.head(TOP_N)\n",
    "        top_neg = s.tail(TOP_N)\n",
    "        n_countries = int((labels_km_best == cl).sum())\n",
    "        summary_rows.append({\n",
    "            \"cluster\": int(cl),\n",
    "            \"top_positive_features\": \", \".join([f\"{i} ({v:.2f})\" for i, v in top_pos.items()]),\n",
    "            \"top_negative_features\": \", \".join([f\"{i} ({v:.2f})\" for i, v in top_neg.items()]),\n",
    "            \"n_countries\": n_countries\n",
    "        })\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"cluster\")\n",
    "    summary_df.to_csv(os.path.join(OUT_DIR, f\"cluster_summary_top_features_{name}.csv\"), index=False)\n",
    "    \n",
    "    # 3) countries_per_cluster.json\n",
    "    countries_per_cluster = (\n",
    "        out_km.groupby(\"cluster_kmeans_umap\")[\"Country\"]\n",
    "        .apply(list)\n",
    "        .reset_index(name=\"countries\")\n",
    "    )\n",
    "    countries_per_cluster = countries_per_cluster.to_dict(orient=\"records\")\n",
    "    with open(os.path.join(OUT_DIR, f\"countries_per_cluster_{name}.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(countries_per_cluster, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"✓ Rapor dosyaları oluşturuldu ({name}):\")\n",
    "    print(f\"   - cluster_profiles_zscores_{name}.csv\")\n",
    "    print(f\"   - cluster_summary_top_features_{name}.csv\")\n",
    "    print(f\"   - countries_per_cluster_{name}.json\")\n",
    "    \n",
    "    # ===========================================================\n",
    "    \n",
    "    # Save main outputs\n",
    "    final_results_hdb[name].to_csv(os.path.join(OUT_DIR, f\"countries_hdbscan_{name}.csv\"), index=False)\n",
    "    final_results_km[name].to_csv(os.path.join(OUT_DIR, f\"countries_kmeans_umap_{name}.csv\"), index=False)\n",
    "    \n",
    "    # Silhouette report\n",
    "    if k_max >= k_min:\n",
    "        pd.DataFrame({\"k\": K, \"silhouette\": sils}).to_csv(os.path.join(OUT_DIR, f\"kmeans_silhouette_{name}.csv\"), index=False)\n",
    "    \n",
    "    reports[name] = {\n",
    "        \"n_countries\": int(n_samples),\n",
    "        \"n_features\": int(n_features),\n",
    "        \"hdbscan_clusters\": int(n_clusters_hdb),\n",
    "        \"hdbscan_noise\": int(n_noise),\n",
    "        \"kmeans_best_k\": km_best_k,\n",
    "        \"kmeans_best_silhouette\": km_best_sil,\n",
    "    }\n",
    "\n",
    "pd.DataFrame.from_dict(reports, orient=\"index\").to_csv(os.path.join(OUT_DIR, \"run_report.csv\"))\n",
    "print(\"\\nTüm işlemler tamamlandı! Çıktılar:\", OUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trendyol_datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
