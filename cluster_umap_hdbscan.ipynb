{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f1e422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3y - %60+ coverage: 70/70 (weeks=157)\n",
      "2y - %60+ coverage: 70/70 (weeks=105)\n",
      "6m - %60+ coverage: 70/70 (weeks=27)\n",
      "Shapes:\n",
      "df_3y: (70, 179) | df_2y: (70, 179) | df_6m: (70, 179)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "\n",
    "df_cleaned = pd.read_csv(\"data/processed/labeled_time_series.csv\")\n",
    "df_cleaned[\"date\"] = pd.to_datetime(df_cleaned[\"date\"])\n",
    "df_cleaned.head()\n",
    "\n",
    "# 0) date format\n",
    "df_cleaned[\"date\"] = pd.to_datetime(df_cleaned[\"date\"])\n",
    "\n",
    "# 1) feature cols\n",
    "META = {\"Country\", \"date\"}\n",
    "feature_cols = [c for c in df_cleaned.columns if c not in META]\n",
    "\n",
    "# 2) rolling cutoffs\n",
    "max_date = df_cleaned[\"date\"].max()\n",
    "cutoff_3y = max_date - pd.DateOffset(years=3)\n",
    "cutoff_2y = max_date - pd.DateOffset(years=2)\n",
    "cutoff_6m = max_date - pd.DateOffset(months=6)\n",
    "\n",
    "df_3y_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_3y].copy()\n",
    "df_2y_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_2y].copy()\n",
    "df_6m_raw = df_cleaned[df_cleaned[\"date\"] >= cutoff_6m].copy()\n",
    "\n",
    "# 3) coverage check (%60+ week coverage)\n",
    "def check_coverage(df_raw, name, threshold=0.6):\n",
    "    total_weeks = df_raw[\"date\"].nunique()\n",
    "    cov = df_raw.groupby(\"Country\")[\"date\"].nunique() / total_weeks\n",
    "    valid = cov[cov >= threshold].index\n",
    "    print(f\"{name} - %{int(threshold*100)}+ coverage: {len(valid)}/{df_raw['Country'].nunique()} (weeks={total_weeks})\")\n",
    "    return valid\n",
    "\n",
    "valid_3y = check_coverage(df_3y_raw, \"3y\")\n",
    "valid_2y = check_coverage(df_2y_raw, \"2y\")\n",
    "valid_6m = check_coverage(df_6m_raw, \"6m\")\n",
    "\n",
    "# 4) aggregate to 1 row per country (median recommended for trends)\n",
    "AGG = \"median\"   # \"mean\" is also ok\n",
    "\n",
    "def agg_country(df_raw, valid, agg=\"median\"):\n",
    "    df_raw = df_raw[df_raw[\"Country\"].isin(valid)]\n",
    "    if agg == \"median\":\n",
    "        return df_raw.groupby(\"Country\")[feature_cols].median().reset_index()\n",
    "    elif agg == \"mean\":\n",
    "        return df_raw.groupby(\"Country\")[feature_cols].mean().reset_index()\n",
    "    else:\n",
    "        raise ValueError(\"agg must be 'median' or 'mean'\")\n",
    "\n",
    "df_3y = agg_country(df_3y_raw, valid_3y, agg=AGG)\n",
    "df_2y = agg_country(df_2y_raw, valid_2y, agg=AGG)\n",
    "df_6m = agg_country(df_6m_raw, valid_6m, agg=AGG)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"df_3y:\", df_3y.shape, \"| df_2y:\", df_2y.shape, \"| df_6m:\", df_6m.shape)\n",
    "\n",
    "datasets = {\"3y\": df_3y, \"2y\": df_2y, \"6m\": df_6m}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20ede91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 3Y WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 3 clusters, noise=17\n",
      "HDBSCAN silhouette (non-noise): 0.411\n",
      "Optimal KMeans: k=2, silhouette=0.346\n",
      "0    34\n",
      "1    36\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 2Y WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 2 clusters, noise=17\n",
      "HDBSCAN silhouette (non-noise): 0.173\n",
      "Optimal KMeans: k=2, silhouette=0.290\n",
      "0    21\n",
      "1    49\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== 6M WINDOW ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP: (70, 10) (neighbors=15, components=10)\n",
      "HDBSCAN: 3 clusters, noise=24\n",
      "HDBSCAN silhouette (non-noise): 0.364\n",
      "Optimal KMeans: k=2, silhouette=0.313\n",
      "0    39\n",
      "1    31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Bitti. Çıktılar: data/processed/clustering_umap_hdbscan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "\n",
    "OUT_DIR = \"data/processed/clustering_umap_hdbscan\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "final_results_hdb = {}\n",
    "final_results_km  = {}\n",
    "reports = {}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n=== {name.upper()} WINDOW ===\")\n",
    "\n",
    "    # --- 1) features (Country 제외)\n",
    "    assert \"Country\" in data.columns\n",
    "    feat_cols = [c for c in data.columns if c != \"Country\"]\n",
    "\n",
    "    # --- 2) numeric + missing\n",
    "    X_raw = data[feat_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = X_raw.fillna(X_raw.median(numeric_only=True)).values\n",
    "\n",
    "    # --- 3) scaling\n",
    "    X_scaled = RobustScaler().fit_transform(X)\n",
    "\n",
    "    # --- 4) UMAP (safe)\n",
    "    n_samples, n_features = X_scaled.shape\n",
    "    n_neighbors  = min(15, max(2, n_samples - 1))\n",
    "    n_components = min(10, n_features)  # 24 feature varsa 10 gayet yeterli\n",
    "\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        n_components=n_components,\n",
    "        min_dist=0.1,\n",
    "        random_state=42,\n",
    "        metric=\"euclidean\"\n",
    "    )\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    print(f\"UMAP: {X_umap.shape} (neighbors={n_neighbors}, components={n_components})\")\n",
    "\n",
    "    # --- 5) HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=5,\n",
    "        min_samples=3,\n",
    "        cluster_selection_method=\"eom\"\n",
    "    )\n",
    "    labels_hdb = clusterer.fit_predict(X_umap)\n",
    "\n",
    "    n_clusters_hdb = len(set(labels_hdb)) - (1 if -1 in labels_hdb else 0)\n",
    "    n_noise = int(np.sum(labels_hdb == -1))\n",
    "    print(f\"HDBSCAN: {n_clusters_hdb} clusters, noise={n_noise}\")\n",
    "\n",
    "    # silhouette for HDBSCAN (ignore noise, only if >=2 clusters)\n",
    "    hdb_sil = None\n",
    "    mask = labels_hdb != -1\n",
    "    if n_clusters_hdb >= 2 and mask.sum() >= 5:\n",
    "        hdb_sil = float(silhouette_score(X_umap[mask], labels_hdb[mask]))\n",
    "        print(f\"HDBSCAN silhouette (non-noise): {hdb_sil:.3f}\")\n",
    "\n",
    "    out_hdb = data[[\"Country\"]].copy()\n",
    "    out_hdb[\"cluster_hdbscan\"] = labels_hdb\n",
    "    final_results_hdb[name] = out_hdb\n",
    "\n",
    "    # --- 6) Optimal KMeans on UMAP\n",
    "    # k must be <= n_samples-1\n",
    "    k_min = 2\n",
    "    k_max = min(15, n_samples - 1)\n",
    "\n",
    "    km_best_k = None\n",
    "    km_best_sil = None\n",
    "    labels_km_best = None\n",
    "\n",
    "    if k_max >= k_min:\n",
    "        K = list(range(k_min, k_max + 1))\n",
    "        sils = []\n",
    "        for k in K:\n",
    "            km = KMeans(n_clusters=k, random_state=42, n_init=25)\n",
    "            labels = km.fit_predict(X_umap)\n",
    "            sils.append(float(silhouette_score(X_umap, labels)))\n",
    "\n",
    "        best_idx = int(np.argmax(sils))\n",
    "        km_best_k = K[best_idx]\n",
    "        km_best_sil = sils[best_idx]\n",
    "\n",
    "        km_best = KMeans(n_clusters=km_best_k, random_state=42, n_init=50)\n",
    "        labels_km_best = km_best.fit_predict(X_umap)\n",
    "\n",
    "        print(f\"Optimal KMeans: k={km_best_k}, silhouette={km_best_sil:.3f}\")\n",
    "        print(pd.Series(labels_km_best).value_counts().sort_index())\n",
    "\n",
    "        out_km = data[[\"Country\"]].copy()\n",
    "        out_km[\"cluster_kmeans_umap\"] = labels_km_best\n",
    "        final_results_km[name] = out_km\n",
    "\n",
    "        # save k-search report\n",
    "        pd.DataFrame({\"k\": K, \"silhouette\": sils}).sort_values(\"silhouette\", ascending=False) \\\n",
    "            .to_csv(os.path.join(OUT_DIR, f\"kmeans_silhouette_{name}.csv\"), index=False)\n",
    "    else:\n",
    "        print(\"KMeans skipped: not enough countries for k>=2.\")\n",
    "        out_km = data[[\"Country\"]].copy()\n",
    "        out_km[\"cluster_kmeans_umap\"] = 0\n",
    "        final_results_km[name] = out_km\n",
    "\n",
    "    # --- 7) save outputs per window\n",
    "    final_results_hdb[name].to_csv(os.path.join(OUT_DIR, f\"countries_hdbscan_{name}.csv\"), index=False)\n",
    "    final_results_km[name].to_csv(os.path.join(OUT_DIR, f\"countries_kmeans_umap_{name}.csv\"), index=False)\n",
    "\n",
    "    reports[name] = {\n",
    "        \"n_countries\": int(n_samples),\n",
    "        \"n_features\": int(n_features),\n",
    "        \"umap_neighbors\": int(n_neighbors),\n",
    "        \"umap_components\": int(n_components),\n",
    "        \"hdbscan_clusters\": int(n_clusters_hdb),\n",
    "        \"hdbscan_noise\": int(n_noise),\n",
    "        \"hdbscan_silhouette_non_noise\": hdb_sil,\n",
    "        \"kmeans_best_k\": km_best_k,\n",
    "        \"kmeans_best_silhouette\": km_best_sil,\n",
    "    }\n",
    "\n",
    "pd.DataFrame.from_dict(reports, orient=\"index\").to_csv(os.path.join(OUT_DIR, \"run_report.csv\"), index=True)\n",
    "print(\"\\nBitti. Çıktılar:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
