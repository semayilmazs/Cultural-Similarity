{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2ae6f-07dc-433c-8c90-36e744c76f83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file loaded.\n",
      " Memory loaded: 71 countries already completed.\n",
      " All countries have already been processed. Task completed!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "from pytrends.request import TrendReq\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# --- 1. SETTINGS ---\n",
    "pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25))\n",
    "\n",
    "# Target Countries List (All)\n",
    "ALL_COUNTRIES = [\n",
    "    'US', 'GB', 'DE', 'FR', 'TR', 'BR', 'IN', 'JP', 'KR', 'RU', \n",
    "    'IT', 'CA', 'AU', 'ES', 'MX', 'ID', 'NL', 'SA', 'CH', 'SE',\n",
    "    'PL', 'BE', 'TH', 'IE', 'AT', 'SG', 'AR', 'NO', 'ZA', 'EG',\n",
    "    'DK', 'MY', 'PH', 'VN', 'FI', 'AE', 'PT', 'CO', 'NZ', 'GR',\n",
    "    'PK', 'UA', 'CL', 'RO', 'CZ', 'HU', 'IL', 'HK', 'TW', 'NG',\n",
    "    'KE', 'BG', 'HR', 'SI', 'SK', 'LT', 'RS', 'UY', 'VE', 'PE',\n",
    "    'EC', 'GH', 'MA', 'LK', 'MM', 'BD', 'EE', 'LV', 'TN', 'BO'\n",
    "]\n",
    "\n",
    "# --- 2. LOAD JSON ---\n",
    "try:\n",
    "    with open('data/raw/keywords_FINAL_2025.json', 'r', encoding='utf-8') as f:\n",
    "        keyword_data = json.load(f)\n",
    "    print(\"JSON file loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: JSON file not found!\")\n",
    "\n",
    "# ID and keyword mapping\n",
    "all_keywords = {}\n",
    "for cat in keyword_data['categories']:\n",
    "    for sg in cat['sub_groups']:\n",
    "        for q in sg['queries']:\n",
    "            if 'topic_id' in q and q['topic_id'].startswith('/'):\n",
    "                all_keywords[q['topic_id']] = q['label']\n",
    "\n",
    "keyword_ids = list(all_keywords.keys())\n",
    "\n",
    "# --- 3. RESUME FROM LAST POINT (Memory Module) ---\n",
    "checkpoint_file = \"data/raw/checkpoint_dataset.csv\"\n",
    "completed_countries = []\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    try:\n",
    "        # Read only the 'Country' column to find completed ones\n",
    "        # chunksize=1000 allows fast reading of large files\n",
    "        chunks = pd.read_csv(checkpoint_file, usecols=['Country'], chunksize=1000)\n",
    "        for chunk in chunks:\n",
    "            completed_countries.extend(chunk['Country'].unique().tolist())\n",
    "        \n",
    "        # Remove duplicates\n",
    "        completed_countries = list(set(completed_countries))\n",
    "        print(f\" Memory loaded: {len(completed_countries)} countries already completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Checkpoint read warning (may be harmless): {e}\")\n",
    "\n",
    "# Select only unfinished countries\n",
    "TARGET_COUNTRIES = [c for c in ALL_COUNTRIES if c not in completed_countries]\n",
    "\n",
    "if not TARGET_COUNTRIES:\n",
    "    print(\" All countries have already been processed. Task completed!\")\n",
    "else:\n",
    "    print(f\" Remaining {len(TARGET_COUNTRIES)} countries to be processed: {TARGET_COUNTRIES}\")\n",
    "\n",
    "# --- 4. DATA FETCH ENGINE (PATIENT TURTLE MODE) ---\n",
    "def fetch_data(countries, keyword_ids):\n",
    "    global pytrends\n",
    "    chunk_size = 5 \n",
    "    keyword_chunks = [keyword_ids[i:i + chunk_size] for i in range(0, len(keyword_ids), chunk_size)]\n",
    "    SAFE_TIMEFRAME = '2022-01-01 2024-12-30'\n",
    "    \n",
    "    # Create headers if the file does not exist\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        dummy_cols = ['Country', 'date'] + list(all_keywords.values())\n",
    "        pd.DataFrame(columns=dummy_cols).to_csv(checkpoint_file, index=False)\n",
    "\n",
    "    for country in tqdm(countries, desc=\"Remaining Countries\"):\n",
    "        country_df = pd.DataFrame()\n",
    "        # Take a long random breath before starting each country\n",
    "        time.sleep(random.randint(5, 10))\n",
    "        \n",
    "        for chunk in keyword_chunks:\n",
    "            basarili = False\n",
    "            deneme_sayisi = 0\n",
    "            \n",
    "            # Persistent retry loop\n",
    "            while not basarili: \n",
    "                try:\n",
    "                    pytrends.build_payload(chunk, timeframe=SAFE_TIMEFRAME, geo=country)\n",
    "                    data = pytrends.interest_over_time()\n",
    "                    \n",
    "                    if not data.empty:\n",
    "                        data = data.drop(columns=['isPartial'], errors='ignore')\n",
    "                        country_df = pd.concat([country_df, data], axis=1)\n",
    "                    \n",
    "                    basarili = True \n",
    "                    # HUMAN BEHAVIOR SIMULATION: wait randomly between 12 and 20 seconds between queries\n",
    "                    wait_human = random.randint(12, 20)\n",
    "                    time.sleep(wait_human)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    if \"429\" in str(e):\n",
    "                        # Rate limit hit. Since IP is static, we must wait LONG.\n",
    "                        # Wait time: 15 minutes (900 seconds) + increase per retry\n",
    "                        wait_time = 900 + (deneme_sayisi * 300)\n",
    "                        now = datetime.now().strftime(\"%H:%M:%S\")\n",
    "                        print(f\"\\n [{now}] Rate limit hit (429). Taking a {int(wait_time/60)} minute break...\")\n",
    "                        print(\"Do not shut down the computer, I will wait and continue.\")\n",
    "                        \n",
    "                        time.sleep(wait_time)\n",
    "                        deneme_sayisi += 1\n",
    "                        \n",
    "                        # Refresh Pytrends instance\n",
    "                        pytrends = TrendReq(hl='en-US', tz=360, timeout=(10,25))\n",
    "                    else:\n",
    "                        print(f\"\\n Critical Error ({country} - {chunk}): {e}\")\n",
    "                        # If not 429, skip this keyword group and exit loop\n",
    "                        basarili = True \n",
    "        \n",
    "        if not country_df.empty:\n",
    "            country_df['Country'] = country\n",
    "            country_df.reset_index(inplace=True) \n",
    "            country_df.rename(columns=all_keywords, inplace=True)\n",
    "            \n",
    "            # INSTANT SAVE (Append Mode)\n",
    "            # Header=False because headers already exist\n",
    "            country_df.to_csv(checkpoint_file, mode='a', header=False, index=False)\n",
    "            # print(f\" {country} completed.\")\n",
    "\n",
    "# --- 5. START ---\n",
    "if len(TARGET_COUNTRIES) > 0:\n",
    "    print(\"Process started... This may take a long time, sit back and relax.\")\n",
    "    fetch_data(TARGET_COUNTRIES, keyword_ids)\n",
    "    print(\"\\n DONE! Check your checkpoint_dataset.csv file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d4b21-8fd0-4205-b45d-7533d213a9a2",
   "metadata": {},
   "source": [
    "\n",
    "During the massive data collection process (over 11,000 rows), some temporary rows might have been recorded with missing country labels due to connection interruptions or API rate limits. \n",
    "\n",
    "In this step, we perform a final sanity check to remove these invalid rows (`NaN` values in the 'Country' column) and save the pristine dataset for the analysis phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5393317b-6e66-4728-a4c7-20a038713ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\selin\\AppData\\Local\\Temp\\ipykernel_6244\\1488499963.py:4: DtypeWarning: Columns (174) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('checkpoint_dataset.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning complete! New file created: final_proje_dataset_CLEAN.csv\n",
      "Final Row Count: 11060\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the checkpoint dataset\n",
    "df = pd.read_csv('checkpoint_dataset.csv')\n",
    "\n",
    "# 2. Remove garbage rows (Drop rows where 'Country' is missing/NaN)\n",
    "df_clean = df.dropna(subset=['Country'])\n",
    "\n",
    "# 3. Save the final, clean version\n",
    "df_clean.to_csv('data/raw/final_proje_dataset_CLEAN.csv', index=False)\n",
    "\n",
    "print(f\"Cleaning complete! New file created: final_proje_dataset_CLEAN.csv\")\n",
    "print(f\"Final Row Count: {len(df_clean)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
